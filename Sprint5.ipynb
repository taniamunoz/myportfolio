{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d391a420-e93c-40f1-871a-2df50b65f6fd",
   "metadata": {},
   "source": [
    "The objective will be to perform a Machine Learning model to predict whether a future procedure will end in arrest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0471f75-7d68-4c3d-ba82-3dddea92897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the libraries that will be used.\n",
    "\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, recall_score, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cea3cbc-f3c8-40fd-a774-1ac7c1e7088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concatenated dataframe has a total of   11825 rows.\n",
      "and  112 columns.\n"
     ]
    }
   ],
   "source": [
    "# 2.- Import and review of the data.\n",
    "# The datasets of the arrests made in 2009 and 2010 are imported. Since the data source comes from the same database, they have the same columns. \n",
    "# Both sets are consolidated and a basic exploration of the data (number of rows/columns, data types, basic statistics, missing cases) is performed.\n",
    "\n",
    "# both dataframes are loaded \n",
    "df_1 = pd.DataFrame(pd.read_csv('2009_1perc.csv'))\n",
    "df_2 = pd.DataFrame(pd.read_csv('2010_1perc.csv'))\n",
    "\n",
    "# the concatenation of the df's is performed in order to unify them\n",
    "df = pd.concat([df_1, df_2],axis=0, ignore_index=True)\n",
    "df.head(5)\n",
    "print(\"The concatenated dataframe has a total of  \", df.shape[0], \"rows.\")\n",
    "print(\"and \", df.shape[1], \"columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b2865a7-4bb9-49d6-a6bd-61c4aa89e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The following are the types of data contained in the df:**\n",
      "Unnamed: 0     int64\n",
      "year           int64\n",
      "pct            int64\n",
      "ser_num        int64\n",
      "datestop       int64\n",
      "               ...  \n",
      "xcoord        object\n",
      "ycoord        object\n",
      "dettypcm      object\n",
      "linecm        object\n",
      "detailcm      object\n",
      "Length: 112, dtype: object\n",
      "\n",
      "**We check which data is unique inside the df:**\n",
      "Unnamed: 0    [178048, 498873, 463573, 43626, 563921, 261097...\n",
      "year                                               [2009, 2010]\n",
      "pct           [41, 108, 43, 77, 110, 14, 67, 75, 34, 113, 60...\n",
      "ser_num       [1779, 5805, 8340, 932, 11224, 5194, 11758, 27...\n",
      "datestop      [4032009, 10292009, 10062009, 1232009, 1213200...\n",
      "                                    ...                        \n",
      "xcoord        [1013067, 1012043, 1017599, 1002625, 1024535, ...\n",
      "ycoord        [0238633, 0212157, 0240200, 0183442, 0209890, ...\n",
      "dettypcm                                                [CM,  ]\n",
      "linecm                                                [1, 1,  ]\n",
      "detailcm      [20, 14, 24, 26, 31, 45, 85, 59, 68, 9, 28, 46...\n",
      "Length: 112, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2.- Import and review of data\n",
    "\n",
    "print(\"**The following are the types of data contained in the df:**\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n**We check which data is unique inside the df:**\")\n",
    "print(df.apply(pd.unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97478caa-204f-4e51-a715-fc57110da16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The following columns have null values:**\n",
      "premname    1\n",
      "stinter     1\n",
      "dtype: int64\n",
      "\n",
      "**Identify the columns containing empty spaces:**\n",
      "Unnamed: 0      0\n",
      "year            0\n",
      "pct             0\n",
      "ser_num         0\n",
      "datestop        0\n",
      "             ... \n",
      "xcoord        417\n",
      "ycoord        417\n",
      "dettypcm        1\n",
      "linecm          1\n",
      "detailcm      830\n",
      "Length: 112, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2.- Import and review of data\n",
    "\n",
    "# We identify the number of missing values in each column.\n",
    "valores_NA = df.isnull().sum()\n",
    "\n",
    "# Filter out only columns with null values\n",
    "total_nulos = valores_NA[valores_NA > 0]\n",
    "print(\"**The following columns have null values:**\")\n",
    "print(total_nulos)\n",
    "\n",
    "# function to count the empty spaces\n",
    "def contar_espacios(x):\n",
    "    if x.dtype == 'object':\n",
    "        return x.str.contains(' ').sum()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print(\"\\n**Identify the columns containing empty spaces:**\")\n",
    "print(df.apply(contar_espacios))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fc64bd-871a-4fc0-b95b-49ec081a6b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below are the basic statistics for the df:\n",
      "           Unnamed: 0          year           pct       ser_num      datestop  \\\n",
      "count   11825.000000  11825.000000  11825.000000  11825.000000  1.182500e+04   \n",
      "mean   295983.814799   2009.508499     68.625624   5697.994334  6.364221e+06   \n",
      "std    170820.705702      0.499949     33.037826   5152.486022  3.429091e+06   \n",
      "min        52.000000   2009.000000      1.000000      1.000000  1.012009e+06   \n",
      "25%    147515.000000   2009.000000     42.000000   2002.000000  3.232009e+06   \n",
      "50%    296652.000000   2010.000000     73.000000   4366.000000  6.162010e+06   \n",
      "75%    443097.000000   2010.000000    102.000000   7840.000000  9.232009e+06   \n",
      "max    601281.000000   2010.000000    123.000000  31694.000000  1.231201e+07   \n",
      "\n",
      "           timestop        perobs       perstop  compyear  comppct  \\\n",
      "count  11825.000000  11825.000000  11825.000000   11825.0  11825.0   \n",
      "mean    1413.725497      2.654391      5.649556       0.0      0.0   \n",
      "std      744.726359      9.759031      9.758535       0.0      0.0   \n",
      "min        0.000000      0.000000      1.000000       0.0      0.0   \n",
      "25%     1000.000000      1.000000      3.000000       0.0      0.0   \n",
      "50%     1605.000000      1.000000      5.000000       0.0      0.0   \n",
      "75%     2035.000000      3.000000      5.000000       0.0      0.0   \n",
      "max     2830.000000    926.000000    857.000000       0.0      0.0   \n",
      "\n",
      "             repcmd        revcmd           dob           age       ht_feet  \\\n",
      "count  11825.000000  11825.000000  1.182500e+04  11825.000000  11825.000000   \n",
      "mean     243.354926    238.986385  7.316195e+06     28.968541      5.183340   \n",
      "std      297.788599    299.394740  3.729206e+06     29.205278      0.398801   \n",
      "min        1.000000      1.000000  1.011900e+06      1.000000      3.000000   \n",
      "25%       70.000000     66.000000  4.121982e+06     19.000000      5.000000   \n",
      "50%      105.000000    103.000000  7.261982e+06     25.000000      5.000000   \n",
      "75%      177.000000    176.000000  1.105199e+07     34.000000      5.000000   \n",
      "max      879.000000    879.000000  1.231200e+07    999.000000      7.000000   \n",
      "\n",
      "            ht_inch        weight  \n",
      "count  11825.000000  11825.000000  \n",
      "mean       6.408034    170.093615  \n",
      "std        3.405503     37.195884  \n",
      "min        0.000000      1.000000  \n",
      "25%        4.000000    150.000000  \n",
      "50%        7.000000    170.000000  \n",
      "75%        9.000000    180.000000  \n",
      "max       11.000000    999.000000  \n"
     ]
    }
   ],
   "source": [
    "# 2.- Import and review of data\n",
    "\n",
    "print(\"\\nBelow are the basic statistics for the df:\\n\", df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023624eb-8c0f-4f37-9e08-f4be2742f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.- Data preprocessing\n",
    "# 3.1 Obtain a list of all categorical variables with between 2 and 99 categories (inclusive). (hint: these are the categorical type variables\n",
    "\n",
    "variables_categoricas = []\n",
    "for columna in df.columns:\n",
    "    if df[columna].dtype == 'object':  # Check if the column is of categorical type\n",
    "        if 2 <= df[columna].nunique() <= 99:  # Check if you have between 2 and 99 categories\n",
    "            variables_categoricas.append(columna)\n",
    "\n",
    "print(\"**The following is a list of the categorical variables:**\\n\", variables_categoricas)\n",
    "\n",
    "# we see the information of the categorical data\n",
    "df_categoricas = df[variables_categoricas]\n",
    "print(\"\\n**Information on categorical variables:**\")\n",
    "df_categoricas.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443aa2b-30d9-472a-ab11-4e1fc73d9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Replace the following missing classes:\n",
    "# if any category of the columns officrid, offshld or offverb is equal to “” change it to 'N' and otherwise leave it as 'Y'.\n",
    "\n",
    "# function that allows to change the column category to N or Y\n",
    "def cambiar_categoria(categoria):\n",
    "    if categoria == \" \":\n",
    "        return 'N'\n",
    "    else:\n",
    "        return 'Y'\n",
    "\n",
    "# first we check how the columns officrid, offshld, offverb are conformed and we see what their unique values are\n",
    "print(\"Columns before applying replacement.\")\n",
    "print(df_categoricas[['officrid', 'offshld', 'offverb']])\n",
    "print(df_categoricas[['officrid', 'offshld', 'offverb']].apply(pd.unique))\n",
    "print(\"******************************************************************\")\n",
    "\n",
    "# then we call the function defined at the beginning of the block to change the category if applicable\n",
    "df_categoricas[['officrid', 'offshld', 'offverb']] = df_categoricas[['officrid', 'offshld', 'offverb']].map(cambiar_categoria)\n",
    "\n",
    "#check how the data is\n",
    "print(\"Columns after applying the replacement.\")\n",
    "print(df_categoricas[['officrid', 'offshld', 'offverb']])\n",
    "print(df_categoricas[['officrid', 'offshld', 'offverb']].apply(pd.unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9456d-ebff-4439-9a79-5333975907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Replace the following missing classes:\n",
    "\n",
    "# if any category in the sector, trhsloc or beat columns equals “” (or NA, depending on how you have categorized the database),\n",
    "# change it to 'U' and otherwise keep its value.\n",
    "# Note, the values mean {N: No, Y: Yes, U: Unknown}\n",
    "\n",
    "# check how the data in the columns 'officrid', 'offshld', 'offverb', 'sector', 'trhsloc', 'beat' are located\n",
    "# identifying the unique values it has\n",
    "print(\"Columns before applying replacement.\")\n",
    "print(df_categoricas[['sector', 'trhsloc', 'beat']].apply(pd.unique))\n",
    "print(\"******************************************************************\")\n",
    "\n",
    "# categories that are found with NAN are filled in with U\n",
    "df_categoricas[['sector', 'trhsloc', 'beat']] = df_categoricas[['sector', 'trhsloc', 'beat']].fillna('U')\n",
    "\n",
    "# the unique values are shown, it is used to validate that the previous replacement has been made\n",
    "print(\"Columns after applying the replacement.\")\n",
    "print(df_categoricas[['sector', 'trhsloc', 'beat']].apply(pd.unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d476e-780a-45d5-a4ba-58e82323a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Transform the columns ht_feet together with ht_inch into a single column (of the form “ht_feet.ht_inch”) called 'meters'.\n",
    "# (hint: transform with the following calculation: meters = (feet+inches)*0.3048)\n",
    "\n",
    "# a function is defined to calculate the meters \n",
    "def calcular_metros(feet, inch):\n",
    "    return (feet + inch / 12) * 0.3048\n",
    "\n",
    "# the function defined above is applied to the ht_feet and ht_inch columns to create the meters column.\n",
    "df['meters'] = df.apply(lambda row: calcular_metros(row['ht_feet'], row['ht_inch']), axis=1)\n",
    "df = df.drop(columns=[\"ht_feet\",\"ht_inch\"])\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd8d28-8f9e-4cad-8e85-1f4b40cf8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Notice that the date comes in a MMDDYYYYYY format in the datestop column. Generate 2 new columns named month and\n",
    "# year columns with only the month and year respectively.\n",
    "\n",
    "# The date format of the datestop column is changed.\n",
    "df['datestop'] = pd.to_datetime(df['datestop'], format='%m%d%Y')\n",
    "\n",
    "# Then the month is extracted and added to the new month column, the same is not done for year because the year column already exists.\n",
    "df['month'] = df['datestop'].dt.month\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641594a9-d7fd-47bf-a3c3-0d6d57ec2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Filter your DataFrame and leave only the columns selected in 3.1, the month, the year, the meters and the age. Then only leave\n",
    "# the records whose ages are between 18 and 100 years inclusive.\n",
    "\n",
    "# Filter to leave only the columns in 3.1 plus 'month', 'year', 'meters' and 'age'.\n",
    "df_filtrado = pd.concat([df_categoricas, df[['month', 'year', 'meters', 'age']]], axis=1)\n",
    "\n",
    "# Filter to leave only records with ages between 18 and 100 years old\n",
    "df_filtrado = df_filtrado[(df_filtrado['age'] >= 18) & (df_filtrado['age'] <= 100)]\n",
    "\n",
    "print(\"**We visualize the filtered df with the columns categories plus month, year, meters and age:**\")\n",
    "print(df_filtrado.head(3))\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "print(\"**We visualize the unique values of the df to see if there are still empty values:**\")\n",
    "print(df_filtrado.apply(pd.unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa2a5b-044e-4ed3-8aa6-8277d557a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis\n",
    "# 4.1 The response variable is studied on its own (arstmade), with the help of a graph.\n",
    "\n",
    "plt.bar(df_filtrado['arstmade'].value_counts().index, df_filtrado['arstmade'].value_counts().values)\n",
    "\n",
    "plt.title(\"Proceeding ends in arrest?\")\n",
    "plt.xlabel(\"arstmade\")\n",
    "plt.ylabel(\"number of procedures\")\n",
    "plt.show()\n",
    "\n",
    "# Observation:\n",
    "# Thanks to the graph we can notice that there is a noticeable difference between the number of procedures that DO NOT end in arrest vs. \n",
    "# number that do result in arrests. That is, most of the procedures do not end in arrest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79517c-59a2-403a-802f-cd13de6ac4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.- Study the relationship of the behavioral response variable with race, comment.\n",
    "\n",
    "# A crosstab is performed between race and the arrests made.\n",
    "tabla_contingencia = pd.crosstab(df_filtrado['race'], df_filtrado['arstmade'])\n",
    "print (\"Table: Arrests by race\")\n",
    "print (tabla_contingencia)\n",
    "\n",
    "# The table above is plotted to provide a graphical view of the relationship between these variables.\n",
    "tabla_contingencia.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Relationship between arrests made and race\")\n",
    "plt.xlabel(\"Race\")\n",
    "plt.ylabel(\"number of procedures\")\n",
    "plt.show()\n",
    "\n",
    "#Observation:\n",
    "# Thanks to the table and graph above, we can notice that the breed with the highest number of procedures corresponds to category B, \n",
    "# followed by breeds Q and W. On the other hand, breeds I, U and Z have the lowest number of procedures.\n",
    "# For breeds B and Q, these have the highest number of confirmed arrests, followed by breeds W and P, \n",
    "# which would indicate that these breeds are more prone to races are more likely to be arrested. \n",
    "# In the case of race I, it can be seen that it has 0 arrests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe861be-90e9-48e4-bbfd-1c1dae58a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Study the relationship of the behavioral response variable with sex, and comment.\n",
    "\n",
    "# a crosstab is performed between the variables sex and arrests made.\n",
    "tabla_sexo = pd.crosstab(df_filtrado['sex'], df_filtrado['arstmade'])\n",
    "print (\"Table: Arrests by Genre\")\n",
    "print (tabla_sexo)\n",
    "\n",
    "tabla_sexo.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Relationship between arrests made and race\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"number of procedures\")\n",
    "plt.show()\n",
    "\n",
    "# Observation:\n",
    "# We can visualize that the majority of the arrests made correspond to persons of the Male sex, followed by the Female sex.\n",
    "# and then followed by the Z category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49db89-2f62-4793-b71a-42812790fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Study the relationship of the behavioral response variable with sex and age as a whole, comment.\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.scatterplot(x='age', y='arstmade', hue='sex', data=df_filtrado, palette='bright')\n",
    "plt.title(\"Relationship between arrests, gender and age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Number of arrests\")\n",
    "\n",
    "plt.legend(title='Gender', loc='center right')\n",
    "plt.xticks(range(0, max(df_filtrado['age'])+1,10))\n",
    "plt.show()\n",
    "\n",
    "# Observation:\n",
    "# In the graph we can see that there are few procedures over 80 years of age, in addition we can see that the procedures that do not end up \n",
    "# arrest over 60 years of age correspond mostly to the male sex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7ed41-e5f9-4a99-a20c-6a6371a7c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Recode the response variable to 1 and 0. Where 0 is N and 1 is Y.\n",
    "\n",
    "# Create a mapping dictionary\n",
    "mapeo = {'N': 0, 'Y': 1}\n",
    "\n",
    "# Recode the 'answer' column\n",
    "df_filtrado['arstmade_mapeo'] = df_filtrado['arstmade'].map(mapeo)\n",
    "df_filtrado = df_filtrado.drop(columns=['arstmade'])\n",
    "\n",
    "print(\"DataFrame with the recoded response variable:\")\n",
    "print(df_filtrado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be40e92-86f4-4584-81da-b15770ca7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Show on a graph the probability that an individual will be arrested, conditional on gender and race. \n",
    "# What are the ethical implications of some of the conclusions you observe?\n",
    "\n",
    "probabilidad = df_filtrado.groupby(['sex','race'])['arstmade_mapeo'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='race', y='arstmade_mapeo', hue='sex', data=probabilidad)\n",
    "plt.title(\"Probability of arrest by gender and race\")\n",
    "plt.xlabel(\"Race\")\n",
    "plt.ylabel(\"Probability of arrest\")\n",
    "plt.legend(title='Gender', loc='center right')\n",
    "plt.show()\n",
    "\n",
    "# Observation:\n",
    "# In the graph we can see that people belonging to race U whose gender is unknown (Z) have a high probability of being arrested.\n",
    "# As well as people of race Q with unknown gender.\n",
    "# Next come female persons of races B, P, and W.\n",
    "# There is no likelihood of female persons of races U and I to be arrested.\n",
    "# There is also no probability that persons of unknown gender belonging to races A, B, P and W will be arrested.\n",
    "# There is 0 probability that persons of race I of any gender will be arrested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374f174-d264-439f-b781-e8330d4f2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.- Determine whether the police procedure will result in any violent action.\n",
    "# The attributes prefixed with pf (['pf_hands'],['pf_wall'], ['pf_grnd'],['pf_drwep'], ['pf_ptwep'],['pf_baton'],['pf_hcuff'], \n",
    "# ['pf_pepsp'], ['pf_pepsp'] and ['pf_other']) \n",
    "# indicate whether there was physical force used by the officer at the time of the procedure, marked 'Y'.\n",
    "# Generate a new variable called 'violence' which is 1 if in any of the 9 pf variables there was any 'Y', and 0 otherwise. \n",
    "# Then indicate the percentage of cases that ended with violence.\n",
    "\n",
    "columnas_pf = df_filtrado.filter(regex='^pf')\n",
    "\n",
    "# Generate the new variable 'violence'\n",
    "df_filtrado['violencia'] = columnas_pf.apply(lambda row: 1 if 'Y' in row.values else 0, axis=1)\n",
    "\n",
    "# Calculate the percentage of cases with violence\n",
    "porcentaje_violencia = (df_filtrado['violencia'].sum() / len(df_filtrado)) * 100\n",
    "\n",
    "print(\"Percentage of cases that ended in violence:\", porcentaje_violencia, \"%\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4db9a1-81a0-411c-8bfd-524d7d64eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.- Modeling\n",
    "# 6.1 The corresponding dummy variables are generated. Then the train-test sets are generated using the year 2009 for training, \n",
    "# and the year 2010 for testing.\n",
    "\n",
    "# before continuing with the modeling, we will finish cleaning up the data.\n",
    "# then to the addrtyp variable we add an N to the spaces\n",
    "df_filtrado['addrtyp'] = df_filtrado['addrtyp'].replace(\" \", \"N\")\n",
    "\n",
    "# we remove the spaces from the beat column\n",
    "df_filtrado['beat'] = df_filtrado['beat'].replace(\" \", \"N\")\n",
    "\n",
    "# we remove the blanks inside the values of the beat column\n",
    "df_filtrado['beat'] = df_filtrado['beat'].str.replace(' ', '')\n",
    "\n",
    "# As some columns still have empty values, we do a cleanup to remove spaces and empty values from the df.\n",
    "df_filtrado = df_filtrado.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df_filtrado.replace(r'^\\s*$', pd.NA, regex=True, inplace=True)\n",
    "df_filtrado.dropna(inplace=True)\n",
    "\n",
    "# we will eliminate the columns city, dettypcm, linecm because they do not generate information for us\n",
    "df_filtrado = df_filtrado.drop(columns={'city', 'dettypcm', 'linecm'})\n",
    "\n",
    "df_filtrado.info()\n",
    "df_filtrado.apply(pd.unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb873d-a56f-4747-9ab9-0291a5154f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.- Modeling\n",
    "# 6.1 The corresponding dummy variables are generated. Then the train-test sets are generated using the year 2009 for training, \n",
    "# and the year 2010 for testing.\n",
    "\n",
    "# we proceed to eliminate redundant columns\n",
    "df_filtrado = df_filtrado.drop(columns= {'pf_hands', 'pf_wall', 'pf_grnd','pf_drwep', 'pf_ptwep','pf_baton','pf_hcuff','pf_pepsp','pf_other'})\n",
    "\n",
    "#we can group all the data for additional circumstances as we did for the violence base\n",
    "df_filtrado.loc[(df_filtrado[['ac_rept','ac_inves','ac_proxm', 'ac_evasv', 'ac_assoc','ac_cgdir', 'ac_incid', 'ac_time','ac_stsnd', 'ac_other']] == 'Y').max(axis=1), 'AC'] = 1\n",
    "\n",
    "# assign a value of 0 to cases with no violence\n",
    "df_filtrado['AC'] = df_filtrado['AC'].fillna(0)\n",
    "\n",
    "# we eliminate the ac columns since they give the same AC record\n",
    "df_filtrado = df_filtrado.drop(columns = {'ac_rept', 'ac_inves','ac_proxm', 'ac_evasv', 'ac_assoc','ac_cgdir', 'ac_incid', 'ac_time','ac_stsnd', 'ac_other'})\n",
    "\n",
    "df_filtrado.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9c9d5-1f48-4f66-92d7-a36e5d342dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure for RF prefixes\n",
    "df_filtrado.loc[(df_filtrado[['rf_vcrim', 'rf_othsw','rf_attir', 'rf_vcact','rf_rfcmp', 'rf_verbl',  'rf_knowl', 'rf_furt', 'rf_bulg']] == 'Y').max(axis=1), 'RF'] = 1\n",
    "\n",
    "# assign a value of 0 to cases with no violence\n",
    "df_filtrado['RF'] = df_filtrado['RF'].fillna(0)\n",
    "\n",
    "# we eliminate the ac columns since they give the same RF register\n",
    "df_filtrado = df_filtrado.drop(columns = {'rf_vcrim', 'rf_othsw','rf_attir', 'rf_vcact','rf_rfcmp', 'rf_verbl',  'rf_knowl', 'rf_furt', 'rf_bulg'})\n",
    "\n",
    "df_filtrado['RF'].unique()\n",
    "df_filtrado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388a35a-6c3f-4b22-b7c4-df38508d9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure for CS prefixes\n",
    "df_filtrado.loc[(df_filtrado[['cs_objcs', 'cs_descr', 'cs_casng', 'cs_lkout', 'cs_cloth', 'cs_drgtr','cs_furtv','cs_vcrim', 'cs_bulge', 'cs_other']] == 'Y').max(axis=1), 'CS'] = 1\n",
    "\n",
    "# assign a value of 0 to cases with no violence\n",
    "df_filtrado['CS'] = df_filtrado['CS'].fillna(0)\n",
    "\n",
    "# we eliminate the ac columns since they give the same CS record\n",
    "df_filtrado = df_filtrado.drop(columns = {'cs_objcs', 'cs_descr', 'cs_casng', 'cs_lkout', 'cs_cloth', 'cs_drgtr','cs_furtv','cs_vcrim', 'cs_bulge', 'cs_other'})\n",
    "\n",
    "df_filtrado['CS'].unique()\n",
    "df_filtrado.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f2d18-4992-48b8-adc8-be57d48401f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure for SB prefixes\n",
    "df_filtrado.loc[(df_filtrado[['sb_hdobj', 'sb_outln', 'sb_admis', 'sb_other']] == 'Y').max(axis=1), 'SB'] = 1\n",
    "\n",
    "# assign a value of 0 to cases with no violence\n",
    "df_filtrado['SB'] = df_filtrado['SB'].fillna(0)\n",
    "\n",
    "# we eliminate the ac columns since they give the same CS record\n",
    "df_filtrado = df_filtrado.drop(columns = {'sb_hdobj', 'sb_outln', 'sb_admis', 'sb_other'})\n",
    "\n",
    "df_filtrado['SB'].unique()\n",
    "df_filtrado.info() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda34d9-f064-49fd-9ef4-3d566a5e71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then all categorical variables with two values we change them to 0 and 1\n",
    "df_filtrado.apply(pd.unique)\n",
    "\n",
    "df_filtrado['recstat'] = df_filtrado['recstat'].replace({'1':1, 'A':0})\n",
    "df_filtrado['inout'] = df_filtrado['inout'].replace({'I':1, '0':0})\n",
    "df_filtrado['explnstp'] = df_filtrado['explnstp'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['othpers'] = df_filtrado['othpers'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['sumissue'] = df_filtrado['sumissue'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['offunif'] = df_filtrado['offunif'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['officrid'] = df_filtrado['officrid'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['frisked'] = df_filtrado['frisked'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['searched'] = df_filtrado['searched'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['contrabn'] = df_filtrado['contrabn'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['pistol'] = df_filtrado['pistol'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['riflshot'] = df_filtrado['riflshot'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['asltweap'] = df_filtrado['asltweap'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['knifcuti'] = df_filtrado['knifcuti'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['othrweap'] = df_filtrado['othrweap'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['radio'] = df_filtrado['radio'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['offverb'] = df_filtrado['offverb'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['offshld'] = df_filtrado['offshld'].replace({'Y':1, 'N':0})\n",
    "df_filtrado['addrtyp'] = df_filtrado['addrtyp'].replace({'L':1, 'N':0})\n",
    "\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322d8d7-1c3a-4deb-a736-3de4ee5f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we eliminate the columns that have only one CS data\n",
    "df_filtrado = df_filtrado.drop(columns = {'CS'})\n",
    "df_filtrado.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8605074-d702-4a6a-95ee-cb472de2a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.- Modeling\n",
    "# 6.1 The corresponding dummy variables are generated. Then the train-test sets are generated using the year 2009 for training, \n",
    "# and the year 2010 for testing.\n",
    "\n",
    "# the data is clean so we will proceed to calculate the rest of the things we are asked to do\n",
    "\n",
    "listado_dummies=[\"trhsloc\",\"typeofid\",\"sex\",\"race\",\"haircolr\",\"eyecolor\",\"build\",\"sector\",\"beat\", 'inout']\n",
    "df_clean = pd.get_dummies(df_filtrado, columns=listado_dummies, dtype=int)\n",
    "df_clean.head(5)\n",
    "\n",
    "df_clean.apply(contar_espacios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354ae42-5141-4fe9-9283-c8fa00308a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.- Modelación\n",
    "# 6.1 Luego genere los sets de train-test utilizando el año 2009 para entrenar, y el año 2010 para testear.\n",
    "\n",
    "# Separate the data into training and test sets.\n",
    "train = df_clean.loc[df_clean['year'] == 2009]\n",
    "test = df_clean.loc[df_clean['year'] == 2010]\n",
    "\n",
    "# now we eliminate the year column of both df, because the test data has the 2009 data and the test data has the 2010 data.\n",
    "train = train.drop(columns = ['year'])\n",
    "test = test.drop(columns = ['year'])\n",
    "\n",
    "X_train = train.drop(columns = ['arstmade_mapeo'], axis=1)\n",
    "y_train = train['arstmade_mapeo']\n",
    "\n",
    "X_test = test.drop(columns = ['arstmade_mapeo'], axis=1)\n",
    "y_test = test['arstmade_mapeo']\n",
    "\n",
    "# # then we revisit the data in the training data for X\n",
    "X_train.apply(pd.unique)\n",
    "\n",
    "# we eliminate the columns that have only one value, since they will not help us in training the model.\n",
    "X_train = X_train.drop(columns = {'addrtyp', 'eyecolor_VI', 'beat_27', 'beat_30'})\n",
    "X_test = X_test.drop(columns = {'addrtyp', 'eyecolor_VI', 'beat_27', 'beat_30'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f7f84-eb7d-4720-80d6-df9dba8db6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.- Modeling\n",
    "# 6.1 Then generate the train-test sets using year 2009 for training, and year 2010 for testing.\n",
    "\n",
    "# We create the climber\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# adjusting the scaler to the training data and transforming the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9141bc0-af29-4ba8-8015-1caaf6398563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function that prints the confusion matrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa574e-7b03-492e-8680-b066e1ba3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Train 4 classification models, report the best model under some criterion. Cross validation is used to test different \n",
    "# hyperparameters for each model.\n",
    "\n",
    "# Model 1\n",
    "\n",
    "# we create a random forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# we fit the model to the training data\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# model cross validation\n",
    "RFC_CV = cross_val_score(rf_model, X_train_scaled, y_train, cv=5).mean()\n",
    "print(\"**Cross-validation of the random forest model:\", RFC_CV, \"**\")\n",
    "\n",
    "# Hyperparameter setting\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': list(range(50, 1501)), \n",
    "    'max_depth': list(range(1, 120)) \n",
    "}\n",
    "\n",
    "# Randomized to find the best hyperparameter\n",
    "rand_search_rf = RandomizedSearchCV(rf_model,\n",
    "                                 param_distributions = param_dist,\n",
    "                                 n_iter=5,\n",
    "                                 cv=5)\n",
    "\n",
    "# We adjust the model again to our training base.\n",
    "rand_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# A variable is created for the best model\n",
    "best_rf = rand_search_rf.best_estimator_\n",
    "\n",
    "# The best hyperparameter\n",
    "print('**The best hyperparameter:',  rand_search_rf.best_params_, \"**\")\n",
    "\n",
    "# Predictions with test data\n",
    "y_pred_tree = best_rf.predict(X_test_scaled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "rf_matrix = confusion_matrix(y_test, y_pred_tree, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(rf_matrix, classes=['arstmade_mapeo=1','arstmade_mapeo=0'],normalize= False,  title='Matriz de confusion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724cbd5-46c4-4ef4-b8b0-f754ebd1c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "\n",
    "# we create a KNN model\n",
    "KNN_C= KNeighborsClassifier()\n",
    "KNN_C.fit(X_train_scaled, y_train)\n",
    "\n",
    "# model cross validation\n",
    "KNN_CV=cross_val_score(KNN_C, X_train_scaled, y_train, cv=5).mean()\n",
    "KNN_CV\n",
    "\n",
    "# Hyperparameter setting\n",
    "param_grid = {'n_neighbors': [2, 3]}\n",
    "KNN= KNeighborsClassifier()\n",
    "\n",
    "# Randomized to find the best hyperparameter\n",
    "rand_search_knn = RandomizedSearchCV(KNN,\n",
    "                                 param_distributions = param_grid,\n",
    "                                 n_iter=5,\n",
    "                                 cv=5)\n",
    "rand_search_knn.fit(X_train_scaled, y_train)\n",
    "best_KNN = rand_search_knn.best_estimator_\n",
    "\n",
    "# The best hyperparameter\n",
    "print('# The best hyperparameter:',  rand_search_knn.best_params_)\n",
    "\n",
    "# Predictions with test data\n",
    "y_pred_knn = best_KNN.predict(X_test_scaled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "knn_matrix = confusion_matrix(y_test, y_pred_knn, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(knn_matrix, classes=['arstmade_mapeo=1','arstmade_mapeo=0'],normalize= False,  title='Matriz de confusion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2010e21-8c71-4d84-b8e0-90ed70492c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "\n",
    "# we created a logistic regression model\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Model adjustment\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross validation of the model\n",
    "log_reg_cv = cross_val_score(log_reg, X_train_scaled, y_train, cv=5).mean()\n",
    "print(\"Cross-validation of the logistic regression model:\", log_reg_cv)\n",
    "\n",
    "# Hyperparameter setting\n",
    "log_reg_param_grid = {'C': [0.1, 1, 10], 'penalty': [None, 'l2']}\n",
    "rand_search_log_reg = RandomizedSearchCV(log_reg, param_distributions=log_reg_param_grid, n_iter=5, cv=5)\n",
    "rand_search_log_reg.fit(X_train_scaled, y_train)\n",
    "best_log_reg = rand_search_log_reg.best_estimator_\n",
    "\n",
    "# The best hyperparameter\n",
    "print('The best hyperparameter for logistic regression:', rand_search_log_reg.best_params_)\n",
    "\n",
    "# Predictions with test data\n",
    "y_pred_log_reg = best_log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "log_reg_matrix = confusion_matrix(y_test, y_pred_log_reg, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(log_reg_matrix, classes=['arstmade_mapeo=1','arstmade_mapeo=0'],normalize= False,  title='Matriz de confusion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28ce29-9bd6-415a-9880-4d5f5bc38032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "\n",
    "# we created a Gradient Boosting model\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Model adjustment\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross validation of the model\n",
    "gb_cv = cross_val_score(gb_clf, X_train_scaled, y_train, cv=5).mean()\n",
    "print(\"Cross-validation of Gradient Boosting:\", gb_cv)\n",
    "\n",
    "# Hyperparameter setting\n",
    "gb_param_grid = {'n_estimators': [50, 100, 150],\n",
    "                 'learning_rate': [0.05, 0.1, 0.2],\n",
    "                 'max_depth': [3, 5, 7]}\n",
    "rand_search_gb = RandomizedSearchCV(gb_clf, param_distributions=gb_param_grid, n_iter=5, cv=5)\n",
    "rand_search_gb.fit(X_train_scaled, y_train)\n",
    "best_gb = rand_search_gb.best_estimator_\n",
    "\n",
    "# The best hyperparameter\n",
    "print('The best hyperparameter for Gradient Boosting:', rand_search_gb.best_params_)\n",
    "\n",
    "# Predictions with test data\n",
    "y_pred_gb = best_gb.predict(X_test_scaled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "gb_matrix = confusion_matrix(y_test, y_pred_gb, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(gb_matrix, classes=['arstmade_mapeo=1','arstmade_mapeo=0'],normalize= False,  title='confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3e324-4be0-4a52-a2f7-106f3c72db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the models\n",
    "\n",
    "# Classification report model 1: random forest\n",
    "rforest_metricas = classification_report(y_test, y_pred_tree)\n",
    "print(\"Metric Model 1: Random Forest\")\n",
    "print(rforest_metricas)\n",
    "\n",
    "# Classification report model 2: knn\n",
    "KKM_metricas = classification_report(y_test, y_pred_knn)\n",
    "print(\"Metricas model 2: KNN\")\n",
    "print(KKM_metricas)\n",
    "\n",
    "# Classification report model 3: logistic regression\n",
    "log_reg_metricas = classification_report(y_test, y_pred_log_reg)\n",
    "print(\"Metrics Model 3: Logistic Regression\")\n",
    "print(log_reg_metricas)\n",
    "\n",
    "# Model 4 Classification Report: Gradient Boosting\n",
    "gb_metricas = classification_report(y_test, y_pred_gb)\n",
    "print(\"Metric model 4: Gradient Boosting\")\n",
    "print(gb_metricas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
